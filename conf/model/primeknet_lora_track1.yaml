# Track 1: PW-LoRA + DW-Freeze (Baseline Strategy)
#
# Strategy Overview:
# - Pointwise Conv: LoRA adaptation (rank=8)
# - Depthwise Conv: FROZEN (preserves pre-trained spatial patterns)
# - Critical Scalars: UNFROZEN (scale, beta, slope - essential for residuals/gating)
# - Normalization: UNFROZEN (InstanceNorm2d, LayerNorm1d - domain adaptation)
# - ConvTranspose: Full FT with low LR (upsampling layers)
#
# Best for:
# - Baseline experiments
# - Safe fine-tuning when preserving pre-trained spatial features is critical
# - Quick iterations
#
# Expected Parameters:
# - Total: ~2-3M
# - Trainable: ~500K (15-18%)

model_lib: primeknet
model_class: PrimeKnet
input_type: bcs

param:
  # STFT parameters
  win_len: 400
  hop_len: 100
  fft_len: 400

  # PrimeKnet architecture
  compress_factor: 0.3
  dense_channel: 64
  sigmoid_beta: 2.0
  num_tsblock: 4
  dense_depth: 4
  time_dw_kernel_size: 3
  time_block_kernel: [3, 11, 23, 31]
  freq_block_kernel: [3, 11, 23, 31]
  time_block_num: 2
  freq_block_num: 2
  causal: false
  encoder_padding_ratio: [0.5, 0.5]
  decoder_padding_ratio: [0.5, 0.5]
  infer_type: mapping

# ============================================================================
# LoRA Configuration - Track 1
# ============================================================================
lora:
  enabled: true
  strategy: track1              # PW-LoRA + DW-Freeze

  # Pointwise LoRA hyperparameters
  rank: 8                       # Rank for pointwise convolutions
  alpha: 16                     # Alpha for pointwise (2 * rank)
  dropout: 0.0                  # Dropout for LoRA path

  # Learning rates
  lr: 1.0e-4                    # LoRA + Normalization + Scalars
  conv_transpose_lr: 5.0e-5     # ConvTranspose (Full FT, low LR)

  # Warmup scheduler
  warmup_epochs: 5              # Number of warmup epochs
  warmup_start_factor: 0.2      # Initial LR multiplier (LR starts at lr * 0.2)

  # Target modules (will be filtered by strategy)
  target_modules:
    - Conv1d
    - Conv2d
