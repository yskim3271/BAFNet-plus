# Track 2: PW-LoRA + DW-Full + Differential LR (High Performance Strategy)
#
# Strategy Overview:
# - Pointwise Conv: LoRA adaptation (rank=8)
# - Depthwise Conv: FULL FINE-TUNING with very low LR (1e-5, 10x lower than LoRA)
# - Critical Scalars: UNFROZEN (scale, beta, slope - essential for residuals/gating)
# - Normalization: UNFROZEN (InstanceNorm2d, LayerNorm1d - domain adaptation)
# - ConvTranspose: Full FT with low LR (upsampling layers)
#
# Key Feature: Differential Learning Rates
# - LoRA: 1e-4 (new parameters, can learn aggressively)
# - Depthwise: 1e-5 (pre-trained, needs protection)
# - Normalization/Scalars: 1e-4 (domain adaptation)
# - ConvTranspose: 5e-5 (upsampling, moderate)
#
# Best for:
# - Maximum performance
# - Production deployment
# - When spatial diversity is critical (e.g., fine-grained audio features)
#
# Expected Parameters:
# - Total: ~2-3M
# - Trainable: ~510K (15.5-18.5%)
# - Additional DW params: ~9K (1% of total, but 100% spatial diversity)

model_lib: primeknet
model_class: PrimeKnet
input_type: bcs

param:
  # STFT parameters
  win_len: 400
  hop_len: 100
  fft_len: 400

  # PrimeKnet architecture
  compress_factor: 0.3
  dense_channel: 64
  sigmoid_beta: 2.0
  num_tsblock: 4
  dense_depth: 4
  time_dw_kernel_size: 3
  time_block_kernel: [3, 11, 23, 31]
  freq_block_kernel: [3, 11, 23, 31]
  time_block_num: 2
  freq_block_num: 2
  causal: false
  encoder_padding_ratio: [0.5, 0.5]
  decoder_padding_ratio: [0.5, 0.5]
  infer_type: mapping

# ============================================================================
# LoRA Configuration - Track 2
# ============================================================================
lora:
  enabled: true
  strategy: track2              # PW-LoRA + DW-Full + Differential LR

  # Pointwise LoRA hyperparameters
  rank: 8                       # Rank for pointwise convolutions
  alpha: 16                     # Alpha for pointwise (2 * rank)
  dropout: 0.0                  # Dropout for LoRA path

  # Differential Learning Rates (CRITICAL for Track 2!)
  lr: 1.0e-4                    # LoRA + Normalization + Scalars (new/adaptive params)
  dw_lr: 1.0e-5                 # Depthwise (pre-trained, needs protection, 10x lower!)
  conv_transpose_lr: 5.0e-5     # ConvTranspose (moderate)

  # Warmup scheduler
  warmup_epochs: 5              # Number of warmup epochs
  warmup_start_factor: 0.2      # Initial LR multiplier (LR starts at lr * 0.2)

  # Target modules
  target_modules:
    - Conv1d
    - Conv2d
